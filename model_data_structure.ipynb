{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"semiotic_10000.txt\") as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"semiotic_10_20000.txt\") as f:\n",
    "    content20 = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content20 = [x.strip() for x in content20] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"semiotic_20_30000.txt\") as f:\n",
    "    content30 = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content30 = [x.strip() for x in content30] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"semiotic_30_40000.txt\") as f:\n",
    "    content40 = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content40 = [x.strip() for x in content40] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"semiotic_rest.txt\") as f:\n",
    "    contentrest = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "contentrest = [x.strip() for x in contentrest] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"semiotic_sample.txt\") as f:\n",
    "    contentsample = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "contentsample = [x.strip() for x in contentsample] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_all = content + content20 + content30 + content40 + contentrest + contentsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40006"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "semiotic_classes = ['PUNCT', 'CARDINAL', 'ORDINAL', 'LETTERS', \n",
    "                    'DATE', 'TIME', 'MEASURE', 'SYMB', 'ABBR', \n",
    "                    'WLINK', 'DECIMAL', 'SPORT', 'RNUM', \n",
    "                    'FRACTION', 'DIGIT', 'TELEPHONE', 'DEL', 'MONEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_sentence(sent, nr):\n",
    "    # non-standard words are pulled out of the sentences\n",
    "    sent1 = re.split('\\}\\s?|\\s?\\{', sent)\n",
    "    # the whitespaces are deleted\n",
    "    sent2 = list(filter(None, sent1))\n",
    "    # put all into lists\n",
    "    sent3 = [word.split(\", \") for word in sent2]\n",
    "    # split the plain words and strip of whitespaces\n",
    "    sent4 = [subsent[0].split() if len(subsent) == 1 else subsent for subsent in sent3]\n",
    "    sent5 = [[word.strip() for word in sentence] for sentence in sent4]\n",
    "    \n",
    "    # append a PLAIN class to the plain words\n",
    "    for sent in sent5:\n",
    "        if sent[-1] not in semiotic_classes:\n",
    "            sent.append('PLAIN')\n",
    "            \n",
    "    sent6 = []\n",
    "    # Add the PLAIN class to each standard word\n",
    "    for sent in sent5:\n",
    "        if sent[-1] == 'PLAIN':\n",
    "            for word in sent[:-1]:\n",
    "                sublist = []\n",
    "                sublist.extend([word, word, 'PLAIN'])\n",
    "                sent6.append(sublist)\n",
    "        else:\n",
    "            sent6.append(sent)\n",
    "    \n",
    "    # insert sentence and word numbers\n",
    "    for i in range(len(sent6)):\n",
    "        sent6[i].insert(0, i)\n",
    "        sent6[i].insert(0, nr)\n",
    "            \n",
    "    return sent6\n",
    "\n",
    "# setja setninganúmer sem args hér\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe(data):\n",
    "    all_data = []\n",
    "    for i in range(len(data)):\n",
    "        tmp = handle_sentence(data[i], i+1)\n",
    "        all_data.append(tmp)\n",
    "    flat_data = [item for sublist in all_data for item in sublist]\n",
    "    df = pd.DataFrame(flat_data, columns = ['sentence_id', 'token_id', 'before', 'after', 'semiotic'])\n",
    "    df = df[['sentence_id', 'token_id', 'semiotic', 'before', 'after']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df_all = make_dataframe(content_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_with_sport = list(content_df_all[content_df_all['semiotic'] == 'SPORT']['sentence_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_sport = content_df_all[content_df_all['sentence_id'].isin(sentences_with_sport)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_other = content_df_all[~content_df_all['sentence_id'].isin(sentences_with_sport)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_sport.to_csv(\"dataframe_sport_0503.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_other.to_csv(\"dataframe_other_0503.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df_all.to_csv(\"dataframe_all_0802.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
